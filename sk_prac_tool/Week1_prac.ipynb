{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b87cadb-d513-4303-baee-a37b6f938e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key found and looks good so far!\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables in a file called .env\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Check the key\n",
    "\n",
    "if not api_key:\n",
    "    print(\"No API key was found - please head over to the troubleshooting notebook in this folder to identify & fix!\")\n",
    "elif not api_key.startswith(\"sk-proj-\"):\n",
    "    print(\"An API key was found, but it doesn't start sk-proj-; please check you're using the right key - see troubleshooting notebook\")\n",
    "elif api_key.strip() != api_key:\n",
    "    print(\"An API key was found, but it looks like it might have space or tab characters at the start or end - please remove them - see troubleshooting notebook\")\n",
    "else:\n",
    "    print(\"API key found and looks good so far!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00743dac-0e70-45b7-879a-d7293a6f68a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Here's a concise summary of the document content you provided:\n",
      "\n",
      "---\n",
      "\n",
      "**Title:** Databricks Certified Data Engineer Professional — Complete Study Guide (Expanded)\n",
      "\n",
      "**Overview:**\n",
      "An exam-focused, practical guide covering essential concepts, examples, tips, and checklists for the Databricks Certified Data Engineer Professional exam.\n",
      "\n",
      "---\n",
      "\n",
      "### PART 1 — FOUNDATIONS & ARCHITECTURE\n",
      "\n",
      "**Lakehouse & Databricks Overview:**\n",
      "- Lakehouse combines data lake flexibility and data warehouse management via Delta Lake.\n",
      "- **Control plane:** Managed by Databricks; includes UI, workspace, job orchestration, Unity Catalog.\n",
      "- **Compute plane:** Customer cloud components; clusters, cloud storage (S3, ADLS, GCS).\n",
      "- **Analogy:** Lakehouse = a library combining raw manuscripts (lake) and curated books (tables), with a librarian (Unity Catalog).\n",
      "- **Exam tip:** Know roles of control vs compute plane and metadata location.\n",
      "\n",
      "**Delta Lake Core Concepts:**\n",
      "- **ACID Transactions:** Managed via transaction log (`_delta_log`)\n",
      "- **Transaction Log:** Stores JSON+Parquet; supports time travel.\n",
      "- **Time Travel:** Query previous versions or by timestamp.\n",
      "- **VACUUM:** Cleans old files, with retention considerations.\n",
      "- **Schema Enforcement & Evolution:** `mergeSchema`, `overwriteSchema`.\n",
      "- **Optimistic Concurrency Control:** Ensures safe concurrent writes.\n",
      "- **Commands:** Examples provided for history, restore, vacuum, and changing properties.\n",
      "- **Analogy:** Delta log = change journal, like draft edits/history.\n",
      "\n",
      "---\n",
      "\n",
      "### PART 2 — CHANGE DATA: CDC & CDF\n",
      "\n",
      "**CDC (Change Data Capture):**\n",
      "- Typically using `MERGE INTO` with source snapshots or CDC flags.\n",
      "- Pattern involves deduplication, identifying changes, and `MERGE` operations.\n",
      "- **Sample Pattern:**\n",
      "  ```\n",
      "  MERGE INTO target t\n",
      "  USING source s\n",
      "  ON t.key = s.key\n",
      "  WHEN MATCHED AND THEN UPDATE SET *\n",
      "  WHEN NOT MATCHED THEN INSERT *\n",
      "  ```\n",
      "- **Tip:** Pre-process source data (e.g., using `ROW_NUMBER()`) to handle multiple matches.\n",
      "\n",
      "**Change Data Feed (CDF):**\n",
      "- Enable per table: `ALTER TABLE t SET TBLPROPERTIES (delta.enableChangeDataFeed = true)`.\n",
      "- Facilitates reading change data for downstream processing.\n",
      "\n",
      "---\n",
      "\n",
      "Would you like a more detailed breakdown of any section, or assistance with specific concepts or commands?\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Create your prompts\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are a snarky assistant that analyzes the contents of a website,\n",
    "and provides a short, snarky, humorous summary, ignoring text that might be navigation related.\n",
    "Respond in markdown. Do not wrap the markdown in a code block - respond just with the markdown.\n",
    "\"\"\"\n",
    "user_prompt = \"\"\"\n",
    "Here are the contents of a document\n",
    "    Databricks Certified Data Engineer Professional — Complete Study\n",
    "Guide (Expanded)\n",
    "# Databricks Certified Data Engineer Professional — Complete Study Guide (Expanded)\n",
    "This document is an expanded, exam-focused, hands-on friendly guide covering all essential topics for the Databricks\n",
    "Certified Data Engineer Professional exam. It includes concepts, code-level examples, layman analogies, exam tips,\n",
    "and practical checklists.\n",
    "---\n",
    "## PART 1 — FOUNDATIONS & ARCHITECTURE\n",
    "### Lakehouse & Databricks Overview\n",
    "- **Lakehouse** merges data lake flexibility with data warehouse management. Databricks implements the Lakehouse\n",
    "using Delta Lake on cloud storage.\n",
    "- **Control plane** (Databricks-managed): user interface, workspace, job orchestration, Unity Catalog control.\n",
    "**Compute plane** (customer cloud): clusters, DBFS, cloud storage (S3/ADLS/GCS).\n",
    "**Layman analogy:** Lakehouse = a modern library: raw manuscripts in basement (lake), curated books on shelves\n",
    "(gold tables), librarian catalog (Unity Catalog).\n",
    "**Exam tip:** Know the roles of control plane vs compute plane and where metadata lives.\n",
    "---\n",
    "### Delta Lake Core Concepts\n",
    "- **ACID transactions**: transaction log (`_delta_log`) ensures commits are atomic and durable.\n",
    "- **Delta log**: JSON + Parquet transaction log; time travel uses versions recorded here.\n",
    "- **Time travel**: `SELECT * FROM table VERSION AS OF n` or `TIMESTAMP AS OF '...'`.\n",
    "- **VACUUM**: removes old files; retains data for default retention; be careful with retention windows.\n",
    "- **Schema enforcement** and **schema evolution**: `mergeSchema`, `overwriteSchema`, `addNewColumns`.\n",
    "- **Optimistic Concurrency Control (OCC)**: concurrent writers create commits, Delta verifies against latest state and\n",
    "throws conflicts.\n",
    "**Commands:**\n",
    "```\n",
    "DESCRIBE HISTORY mydb.mytable;\n",
    "RESTORE TABLE mydb.mytable TO VERSION AS OF 3;\n",
    "VACUUM mydb.mytable RETAIN 168 HOURS;\n",
    "ALTER TABLE mydb.mytable SET TBLPROPERTIES ('delta.enableChangeDataFeed' = true);\n",
    "```\n",
    "**Layman analogy:** Delta log is a journal of changes — like saved drafts + edits history for a document.\n",
    "---\n",
    "## PART 2 — CHANGE DATA: CDC & CDF\n",
    "### CDC (Change Data Capture)\n",
    "- Implemented typically via `MERGE INTO` operations using source snapshot or CDC flags.\n",
    "- Pattern: dedupe source, identify inserts/updates/deletes, `MERGE` into target.\n",
    "**Typical SQL MERGE pattern:**\n",
    "```\n",
    "MERGE INTO target t\n",
    "USING source s\n",
    "ON t.key = s.key\n",
    "WHEN MATCHED AND THEN UPDATE SET *\n",
    "WHEN NOT MATCHED THEN INSERT *\n",
    "```\n",
    "**Exam tip:** Understand how to pre-process source to avoid multiple matches (use ROW_NUMBER over partition).\n",
    "### Change Data Feed (CDF)\n",
    "- Enable per-table: `ALTER TABLE t SET TBLPROPERTIES (delta.enableChangeDataFeed = true)`.\n",
    "- Read changes:\n",
    "\"\"\"\n",
    "\n",
    "from openai import OpenAI\n",
    "openai = OpenAI()\n",
    "\n",
    "# Step 2: Make the messages list\n",
    "messages = [{\"role\": \"system\", \"content\": \"Wonderful, you are a helpful assistant\"},\n",
    "{\"role\": \"user\", \"content\": \"Here is the summary of the document: \" + user_prompt}\n",
    "] # fill this in\n",
    "\n",
    "# Step 3: Call OpenAI\n",
    "response = openai.chat.completions.create(model = \"gpt-4.1-nano\", messages = messages)\n",
    "\n",
    "# Step 4: print the result\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0fec649",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a snarky assistant that analyzes the contents of a website,\n",
    "and provides a short, snarky, humorous summary, ignoring text that might be navigation related.\n",
    "Respond in markdown. Do not wrap the markdown in a code block - respond just with the markdown.\n",
    "\"\"\"\n",
    "\n",
    "user_prompt = \"\"\"\n",
    "Here are the contents of a website.\n",
    "Provide a short summary of this website.\n",
    "If it includes news or announcements, then summarize these too.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def messages_for(website):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt + website}\n",
    "    ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "218411f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scraper'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Let's try out this utility\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscraper\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m fetch_website_contents\n\u001b[32m      5\u001b[39m ka= fetch_website_contents(\u001b[33m\"\u001b[39m\u001b[33mhttps://blog.khanacademy.org/\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(ka)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'scraper'"
     ]
    }
   ],
   "source": [
    "# Let's try out this utility\n",
    "\n",
    "from scraper import fetch_website_contents\n",
    "\n",
    "ka= fetch_website_contents(\"https://blog.khanacademy.org/\")\n",
    "print(ka)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb4848b",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_for(\"https://blog.khanacademy.org/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf59770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(url):\n",
    "    website = fetch_website_contents(url)  #pyright: ignore[reportUndefinedVariable]\n",
    "    response = openai.chat.completions.create(model = \"gpt-4.1-nano\", messages = messages_for(website))\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3a289a",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize(\"https://blog.khanacademy.org/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f283f9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_summary(url):\n",
    "    summary = summarize(url)\n",
    "    display(Markdown(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df462df",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_summary(\"https://blog.khanacademy.org/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
