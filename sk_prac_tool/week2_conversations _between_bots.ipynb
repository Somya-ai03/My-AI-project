{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06cf3063-9f3e-4551-a0d5-f08d9cabb927",
   "metadata": {},
   "source": [
    "# Welcome to Week 2!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9813eb2f",
   "metadata": {},
   "source": [
    "Conversations between the bots- using openai and ollama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de23bb9e-37c5-4377-9a82-d7b6c648eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0abffac",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985a859a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to OpenAI client library\n",
    "# A thin wrapper around calls to HTTP endpoints\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "# For Gemini, DeepSeek and Groq, we can use the OpenAI python client\n",
    "# Because Google and DeepSeek have endpoints compatible with OpenAI\n",
    "# And OpenAI allows you to change the base_url\n",
    "\n",
    "\n",
    "ollama_url = \"http://localhost:11434/v1\"\n",
    "\n",
    "\n",
    "ollama = OpenAI(api_key=\"ollama\", base_url=ollama_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988d59f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from openai import OpenAI\n",
    "\n",
    "# -------- CONFIG --------\n",
    "gpt_model = \"gpt-5-nano\"\n",
    "ollama_model = \"llama3.2:1b\"\n",
    "OLLAMA_URL = \"http://localhost:11434/api/chat\"\n",
    "\n",
    "\n",
    "\n",
    "ALEX_SYSTEM = \"\"\"\n",
    "You are Alex, a chatbot who is very argumentative; you disagree with anything in the conversation\n",
    "and you challenge everything, in a snarky way.\n",
    "You are in a conversation with Blake and Charlie.\n",
    "\"\"\"\n",
    "\n",
    "BLAKE_SYSTEM = \"\"\"\n",
    "You are Blake, a very polite, courteous chatbot. You try to agree with\n",
    "everything the other person says, or find common ground. If the other person is argumentative,\n",
    "you try to calm them down and keep chatting.\n",
    "You are in a conversation with Alex and Charlie.\n",
    "\"\"\"\n",
    "\n",
    "CHARLIE_SYSTEM = \"\"\"\n",
    "You are Charlie, a thoughtful mediator chatbot. You try to summarize, clarify misunderstandings,\n",
    "and keep the conversation productive and friendly.\n",
    "You are in a conversation with Alex and Blake.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def build_user_prompt(name: str, conversation: str) -> str:\n",
    "    return f\"\"\"\n",
    "You are {name}, in conversation with the others.\n",
    "The conversation so far is as follows:\n",
    "{conversation}\n",
    "\n",
    "Now, with this, respond with what you would like to say next, as {name}.\n",
    "Only write {name}'s next line, nothing else.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def call_gpt_as_alex(conversation: str) -> str:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": ALEX_SYSTEM},\n",
    "        {\"role\": \"user\", \"content\": build_user_prompt(\"Alex\", conversation)},\n",
    "    ]\n",
    "    response = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages,\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "\n",
    "def call_ollama_blake(conversation: str) -> str:\n",
    "    payload = {\n",
    "        \"model\": ollama_model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": BLAKE_SYSTEM},\n",
    "            {\"role\": \"user\", \"content\": build_user_prompt(\"Blake\", conversation)},\n",
    "        ],\n",
    "        \"stream\": False,\n",
    "    }\n",
    "    r = requests.post(OLLAMA_URL, json=payload)\n",
    "    r.raise_for_status()\n",
    "    return r.json()[\"message\"][\"content\"].strip()\n",
    "\n",
    "\n",
    "def call_ollama_charlie(conversation: str) -> str:\n",
    "    payload = {\n",
    "        \"model\": ollama_model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": CHARLIE_SYSTEM},\n",
    "            {\"role\": \"user\", \"content\": build_user_prompt(\"Charlie\", conversation)},\n",
    "        ],\n",
    "        \"stream\": False,\n",
    "    }\n",
    "    r = requests.post(OLLAMA_URL, json=payload)\n",
    "    r.raise_for_status()\n",
    "    return r.json()[\"message\"][\"content\"].strip()\n",
    "\n",
    "\n",
    "def run_chat(turns: int = 5):\n",
    "    conversation = \"\"\"Alex: Hi there\n",
    "Blake: Hello Alex, pleasure to meet you.\n",
    "Charlie: Yo what's up, chaos crew?\"\"\"\n",
    "\n",
    "    for i in range(turns):\n",
    "        print(f\"\\n--- TURN {i + 1} ---\")\n",
    "\n",
    "        alex = call_gpt_as_alex(conversation)\n",
    "        conversation += f\"\\nAlex: {alex}\"\n",
    "        print(\"Alex:\", alex)\n",
    "\n",
    "        blake = call_ollama_blake(conversation)\n",
    "        conversation += f\"\\nBlake: {blake}\"\n",
    "        print(\"Blake:\", blake)\n",
    "\n",
    "        charlie = call_ollama_charlie(conversation)\n",
    "        conversation += f\"\\nCharlie: {charlie}\"\n",
    "        print(\"Charlie:\", charlie)\n",
    "\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "    print(\"\\nFINAL CONVERSATION:\")\n",
    "    print(conversation)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_chat(turns=5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-engineering (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
