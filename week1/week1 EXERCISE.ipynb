{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
   "metadata": {},
   "source": [
    "# End of week 1 exercise\n",
    "\n",
    "To demonstrate your familiarity with OpenAI API, and also Ollama, build a tool that takes a technical question,  \n",
    "and responds with an explanation. This is a tool that you will be able to use yourself during the course!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from scraper import fetch_website_contents, fetch_website_links\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from openai import OpenAI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "MODEL_GPT = 'gpt-4o-nano'\n",
    "MODEL_LLAMA = 'llama3.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a8d7923c-5f28-4c30-8556-342d7c8497c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key found and looks good so far!\n"
     ]
    }
   ],
   "source": [
    "# set up environment\n",
    "load_dotenv(override=True)\n",
    "api_key=os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "#Check the key\n",
    "\n",
    "if not api_key:\n",
    "    print(\"No API key was found - please head over to the troubleshooting notebook in this folder to identify & fix!\")\n",
    "elif not api_key.startswith(\"sk-proj-\"):\n",
    "    print(\"An API key was found, but it doesn't start sk-proj-; please check you're using the right key - see troubleshooting notebook\")\n",
    "elif api_key.strip() != api_key:\n",
    "    print(\"An API key was found, but it looks like it might have space or tab characters at the start or end - please remove them - see troubleshooting notebook\")\n",
    "else:\n",
    "    print(\"API key found and looks good so far!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "60ce7000-a4a5-4cce-a261-e75ef45063b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The expression `yield from {book.get(\"author\") for book in books if book.get(\"author\")}` is used to generate values from a set comprehension in Python. Here's a breakdown of what it does and why:\n",
       "\n",
       "1. **Set comprehension:** `{book.get(\"author\") for book in books if book.get(\"author\")}`\n",
       "\n",
       "   - It iterates over a collection called `books`, which is presumably a list of dictionaries.\n",
       "   - For each `book`, it retrieves the value associated with the key `\"author\"` using `book.get(\"author\")`.\n",
       "   - It includes only those books where `book.get(\"author\")` is not `None` or a falsy value, thanks to the `if book.get(\"author\")` condition.\n",
       "   - Since it's a set comprehension, duplicates are automatically removed, resulting in a set of unique authors.\n",
       "\n",
       "2. **`yield from`:** This statement is used inside a generator function to delegate part of its operations to another iterable or generator.\n",
       "\n",
       "   - In this case, it takes the set of unique authors generated by the set comprehension.\n",
       "   - It yields each author from that set one by one, as if they are being yielded directly in the generator function.\n",
       "\n",
       "**Why is this used?**\n",
       "\n",
       "- To create a generator that sequentially yields each unique author from the collection of books.\n",
       "- Using `yield from` simplifies the process of yielding all elements from an iterable or set without writing an explicit loop.\n",
       "- Utilizing a set ensures that each author is yielded only once, avoiding duplicates.\n",
       "\n",
       "**In summary:**\n",
       "\n",
       "This code efficiently yields each unique author name from a list of book dictionaries, skipping any entries where the author information is missing, by leveraging a set comprehension and `yield from` within a generator function."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get gpt-4o-mini to answer, with streaming\n",
    "\n",
    "from openai import OpenAI\n",
    "from IPython.display import display, Markdown, update_display\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "question = \"\"\"\n",
    "Please explain what this code does and why:\n",
    "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
    "\"\"\"\n",
    "\n",
    "def get_question_user_prompt(question_type, url):\n",
    "    user_prompt = f\"\"\"\n",
    "You are looking at a question bank called: {question_type}.\n",
    "Here is the URL containing the content: {url}\n",
    "\n",
    "Use this information to answer the following question in markdown without code blocks.\n",
    "\"\"\"\n",
    "    return user_prompt\n",
    "\n",
    "def stream_answer(question_type, url):\n",
    "    stream = openai.chat.completions.create(\n",
    "        model=\"gpt-4.1-nano\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": question},\n",
    "            {\"role\": \"user\", \"content\": get_question_user_prompt(question_type, url)}\n",
    "        ],\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    response = \"\"\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "\n",
    "    for chunk in stream:\n",
    "        delta = chunk.choices[0].delta.content or \"\"\n",
    "        response += delta\n",
    "        update_display(Markdown(response), display_id=display_handle.display_id)\n",
    "\n",
    "# ---- RUN THE FUNCTION ----\n",
    "stream_answer(\"Python_questions\", \"file:///mnt/data/54EC3D4D-3BB9-4FBD-A38F-F0744DC4BDDB.jpeg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8f7c8ea8-4082-4ad0-8751-3301adcf6538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Llama 3.2 to answer\n",
    "##Using Ollama for the above prompt\n",
    "\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434/v1\"\n",
    "\n",
    "ollama = OpenAI(base_url=OLLAMA_BASE_URL, api_key='ollama')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369b1a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling 74701a8c35f6: 100% ▕██████████████████▏ 1.3 GB                         \u001b[K\n",
      "pulling 966de95ca8a6: 100% ▕██████████████████▏ 1.4 KB                         \u001b[K\n",
      "pulling fcc5a6bec9da: 100% ▕██████████████████▏ 7.7 KB                         \u001b[K\n",
      "pulling a70ff7e570d9: 100% ▕██████████████████▏ 6.0 KB                         \u001b[K\n",
      "pulling 4f659a1e86d7: 100% ▕██████████████████▏  485 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "!ollama pull llama3.2:1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bc83999d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the question; type over this to ask something new\n",
    "\n",
    "question = \"\"\"\n",
    "In a Python program that processes book records, the following expression is used:\n",
    "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
    "Explain how this works step-by-step and what its output would look like for a typical list of books.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f9d8105f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "As you take a look at the given image file, it appears to be an URL pointing to an unstyled HTML page in an issue of The New Yorker.\n",
       "\n",
       "The code `yield from {book.get(\"author\") for book in books if book.get(\"author\")}` is likely being used to print out the authors of the books in a list format. This can be seen as an attempt to flatten and print out the author information for each book. \n",
       "\n",
       "Here's how it might work:\n",
       "\n",
       "- It starts with `yield from`, which means it calls another iterable function on this expression (like the generator) and continues until that function exhausts its resources.\n",
       "- The generator yields the value of a dictionary's 'author' key, where those dictionaries are presumably 'book', representing an individual entry in a list of books.\n",
       "\n",
       "The code essentially iterates over each book from the `books` list (`{book for book in books}`) and tries to get a string using `'author'`. If successful, it yields that value."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_question_user_prompt(question_type, url):\n",
    "    user_prompt = f\"\"\"\n",
    "You are looking at a question bank called: {question_type}.\n",
    "Here is the URL containing the content: {url}\n",
    "\n",
    "Use this information to answer the following question in markdown without code blocks.\n",
    "\"\"\"\n",
    "    return user_prompt\n",
    "\n",
    "def stream_answer(question_type, url):\n",
    "    stream = ollama.chat.completions.create(\n",
    "        model=\"llama3.2:1b\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": question},\n",
    "            {\"role\": \"user\", \"content\": get_question_user_prompt(question_type, url)}\n",
    "        ],\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    response = \"\"\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "\n",
    "    for chunk in stream:\n",
    "        delta = chunk.choices[0].delta.content or \"\"\n",
    "        response += delta\n",
    "        update_display(Markdown(response), display_id=display_handle.display_id)\n",
    "\n",
    "# ---- RUN THE FUNCTION ----\n",
    "stream_answer(\"Python_questions\", \"file:///mnt/data/54EC3D4D-3BB9-4FBD-A38F-F0744DC4BDDB.jpeg\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
